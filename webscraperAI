import requests
from bs4 import BeautifulSoup
from transformers import pipeline, AutoTokenizer
import time
from fake_useragent import UserAgent
import random
import os
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext, filedialog
import webbrowser
import json
import csv
import pandas as pd

class IPRoyalSmartScraperApp:
    def __init__(self, root):
        self.root = root
        self.root.title("Smart Web Scraper")
        self.root.geometry("700x600")

        # User agent fallback
        try:
            self.ua = UserAgent()
        except Exception as e:
            self.ua = None
            self.fallback_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
                'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0'
            ]
            self.ua_error = f"Failed to initialize UserAgent: {str(e)}. Using fallback."

        # Initialize models and tokenizer
        self.sentiment_analyzer = pipeline("sentiment-analysis", model="distilbert/distilbert-base-uncased-finetuned-sst-2-english", revision="714eb0f")
        self.summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6", revision="a4f8f3e")
        self.tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
        self.max_tokens = 510
        self.output_dir = "scraped_content"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # GUI Elements
        tk.Label(root, text="Proxy Host:").pack(pady=2)
        self.proxy_host_entry = tk.Entry(root, width=50)
        self.proxy_host_entry.insert(0, "geo.iproyal.com")
        self.proxy_host_entry.pack(pady=2)
        
        tk.Label(root, text="Proxy Port:").pack(pady=2)
        self.proxy_port_entry = tk.Entry(root, width=50)
        self.proxy_port_entry.insert(0, "32325")
        self.proxy_port_entry.pack(pady=2)
        
        tk.Label(root, text="Proxy Username:").pack(pady=2)
        self.proxy_user_entry = tk.Entry(root, width=50)
        self.proxy_user_entry.insert(0, "username")
        self.proxy_user_entry.pack(pady=2)
        
        tk.Label(root, text="Proxy Password:").pack(pady=2)
        self.proxy_pass_entry = tk.Entry(root, width=50, show="*")
        self.proxy_pass_entry.insert(0, "password")
        self.proxy_pass_entry.pack(pady=2)

        tk.Label(root, text="Enter URL(s) (one per line):").pack(pady=5)
        self.url_entry = scrolledtext.ScrolledText(root, width=50, height=5, wrap=tk.WORD)
        self.url_entry.pack(pady=5)
        
        # Buttons frame
        button_frame = tk.Frame(root)
        button_frame.pack(pady=10)
        tk.Button(button_frame, text="Analyze & Scrape", command=self.process_urls).pack(side=tk.LEFT, padx=5)
        tk.Button(button_frame, text="Get a Cheap Proxy Here", command=self.open_iproyal).pack(side=tk.LEFT, padx=5)
        
        # Progress bar
        self.progress = ttk.Progressbar(root, length=400, mode='determinate')
        self.progress.pack(pady=5)
        
        self.output_text = scrolledtext.ScrolledText(root, width=80, height=20, wrap=tk.WORD)
        self.output_text.pack(pady=10)
        self.options = {}

        # Log file setup
        self.log_file = os.path.join(self.output_dir, "scraper_log.txt")
        self.log("App initialized")

    def log(self, message):
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        self.output_text.insert(tk.END, log_message + "\n")
        self.output_text.see(tk.END)
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + "\n")
        self.root.update()

    def open_iproyal(self):
        webbrowser.open("https://iproyal.com/?r=IproyalR")

    def fetch_page(self, url, max_retries=3):
        attempt = 0
        while attempt < max_retries:
            try:
                headers = {'User-Agent': self.ua.random if self.ua else random.choice(self.fallback_agents)}
                response = requests.get(url, proxies=self.proxies, headers=headers, timeout=10)
                response.raise_for_status()
                return response.text
            except requests.exceptions.RequestException as e:
                attempt += 1
                if attempt == max_retries:
                    self.log(f"Failed to fetch page: {str(e)}")
                    try:
                        self.log("Trying without proxy as fallback...")
                        response = requests.get(url, headers=headers, timeout=10)
                        response.raise_for_status()
                        return response.text
                    except Exception as fallback_e:
                        self.log(f"Fallback failed: {str(fallback_e)}")
                        return None
                time.sleep(2 ** attempt)
        return None

    def truncate_text(self, text):
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        if len(tokens) > self.max_tokens:
            tokens = tokens[:self.max_tokens]
            return self.tokenizer.decode(tokens, skip_special_tokens=True)
        return text

    def analyze_content(self, text):
        try:
            truncated_text = self.truncate_text(text)
            sentiment = self.sentiment_analyzer(truncated_text, truncation=True, max_length=512)[0]
            summary = self.summarizer(truncated_text, max_length=130, min_length=30, do_sample=False, truncation=True)[0]['summary_text']
            return {'sentiment': sentiment['label'], 'confidence': sentiment['score'], 'summary': summary}
        except Exception as e:
            self.log(f"Analysis failed: {str(e)}")
            return None

    def identify_scrape_options(self, soup):
        options = {}
        all_text = soup.get_text(separator=' ', strip=True)
        if all_text:
            options['all_text'] = {'type': 'All Text', 'content': all_text, 'desc': 'Entire text content of the page'}
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        if headings:
            options['headings'] = {'type': 'Headings', 'content': ' '.join(h.get_text(strip=True) for h in headings), 'desc': 'All headings (H1-H6) from the page'}
        paragraphs = soup.find_all('p')
        if paragraphs:
            options['paragraphs'] = {'type': 'Paragraphs', 'content': ' '.join(p.get_text(strip=True) for p in paragraphs), 'desc': 'All paragraph text (P tags)'}
        links = soup.find_all('a')
        if links:
            options['links'] = {'type': 'Links Text', 'content': ' '.join(link.get_text(strip=True) for link in links if link.get_text(strip=True)), 'desc': 'Text from all hyperlinks (A tags)'}
        content_divs = soup.find_all('div', class_='content')
        if content_divs:
            options['content_divs'] = {'type': 'Content Divs', 'content': ' '.join(div.get_text(strip=True) for div in content_divs), 'desc': 'Text from divs with class "content"'}
        tables = soup.find_all('table')
        if tables:
            options['tables'] = {'type': 'Tables', 'content': ' '.join(table.get_text(strip=True) for table in tables), 'desc': 'Text from all tables on the page'}
        return options

    def save_to_file(self, url, content_type, content, format='txt'):
        safe_url = ''.join(c if c.isalnum() else '_' for c in url)
        base_filename = f"{self.output_dir}/{safe_url}_{content_type}"
        if format == 'txt':
            filename = f"{base_filename}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
        elif format == 'csv':
            filename = f"{base_filename}.csv"
            df = pd.DataFrame({'URL': [url], 'Type': [content_type], 'Content': [content]})
            df.to_csv(filename, index=False)
        elif format == 'json':
            filename = f"{base_filename}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump({'url': url, 'type': content_type, 'content': content}, f)
        return filename

    def scrape_selection(self, url, page_content, analysis):
        soup = BeautifulSoup(page_content, 'html.parser')
        self.options = self.identify_scrape_options(soup)
        if not self.options:
            self.log("No scrapeable content found.")
            return

        self.log("\nAvailable scraping options:")
        for key, value in self.options.items():
            self.log(f"{value['type']}: {value['desc']} (Preview: {value['content'][:50]}...)")

        dialog = tk.Toplevel(self.root)
        dialog.title("Select Content to Scrape")
        dialog.geometry("500x500")
        tk.Label(dialog, text="Choose what to scrape (select multiple):").pack(pady=5)
        
        # Checkbox variables
        self.scrape_vars = {key: tk.BooleanVar(value=False) for key in self.options}
        for key, value in self.options.items():
            tk.Checkbutton(dialog, text=f"{value['type']} - {value['desc']}", variable=self.scrape_vars[key]).pack(anchor=tk.W, padx=10, pady=2)
        
        tk.Label(dialog, text="Export format:").pack(pady=5)
        self.export_format = tk.StringVar(value='txt')
        tk.Radiobutton(dialog, text="TXT", variable=self.export_format, value='txt').pack(anchor=tk.W, padx=20)
        tk.Radiobutton(dialog, text="CSV", variable=self.export_format, value='csv').pack(anchor=tk.W, padx=20)
        tk.Radiobutton(dialog, text="JSON", variable=self.export_format, value='json').pack(anchor=tk.W, padx=20)
        
        tk.Button(dialog, text="Scrape Selected", command=lambda: self.on_select_multi(url, page_content, dialog)).pack(pady=10)
        tk.Button(dialog, text="Skip", command=dialog.destroy).pack(pady=5)

    def on_select_multi(self, url, page_content, dialog):
        selected_options = [key for key, var in self.scrape_vars.items() if var.get()]
        if not selected_options:
            self.log("No options selected.")
            dialog.destroy()
            return

        format = self.export_format.get()
        for key in selected_options:
            selected = self.options[key]
            self.log(f"\nScraping {selected['type']}...")
            self.log(f"Content: {selected['content'][:200]}..." if len(selected['content']) > 200 else selected['content'])
            self.log(f"Proxy used: {self.proxies['http']}")
            filename = self.save_to_file(url, selected['type'], selected['content'], format)
            self.log(f"Saved to: {filename}")
        dialog.destroy()

    def process_urls(self):
        urls = self.url_entry.get("1.0", tk.END).strip().splitlines()
        if not urls:
            messagebox.showerror("Error", "Please enter at least one URL")
            return

        # Update proxy config
        proxy_host = self.proxy_host_entry.get()
        proxy_port = self.proxy_port_entry.get()
        proxy_user = self.proxy_user_entry.get()
        proxy_pass = self.proxy_pass_entry.get()
        self.proxies = {
            'http': f'socks5://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}',
            'https': f'socks5://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}'
        }
        self.log(f"Updated proxy config: {self.proxies}")

        self.output_text.delete(1.0, tk.END)
        total_urls = len(urls)
        self.progress['maximum'] = total_urls
        for i, url in enumerate(urls, 1):
            url = url.strip()
            if not url:
                continue
            if not url.startswith('http'):
                url = 'https://' + url

            self.log(f"\nProcessing URL {i}/{total_urls}: {url}")
            self.progress['value'] = i
            self.root.update_idletasks()

            page_content = self.fetch_page(url)
            if not page_content:
                continue

            self.log("Analyzing content...")
            analysis = self.analyze_content(page_content)
            if not analysis:
                continue

            self.log(f"Sentiment: {analysis['sentiment']} (Confidence: {analysis['confidence']:.2f})")
            self.log(f"Summary: {analysis['summary']}")
            self.scrape_selection(url, page_content, analysis)

        self.log("\nAll URLs processed!")

if __name__ == "__main__":
    root = tk.Tk()
    app = IPRoyalSmartScraperApp(root)
    root.mainloop()
